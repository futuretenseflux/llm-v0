

# Shared settings
tokenizer_model: "facebook/galactica-6.7b"
sequence_length: 2048
output_dir: "data/processed"

# Dataset URLs
datasets:
  books: "sci-datasets/sci-books"
  code: "sci-datasets/sci-code"
  conv_forum: "sci-datasets/sci_conversations_v1"
  math: "sci-datasets/sci-math"
  papers: "sci-datasets/sci-papers"
  primer: "sci-datasets/sci-primer-v1"
  web: "sci-datasets/fineweb-edu-sci-super"

# Output prefixes
code_output_prefix: "code_unified"
conv_forum_output_prefix: "conv_forum_unified"

# Processing settings
progress_interval: 10000000  # Report progress every 10M tokens
shuffle_buffer_size: 10000   # Number of examples to buffer for shuffling
