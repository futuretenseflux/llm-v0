vocab_size: 151936
dim_model: 1024
dim_k: 128
num_q_heads: 16
group_size: 2
num_decoder_layers: 28
intermediate_size: 3072
eps: 1e-6
dropout: 0.1
batch_size: 32
seq_length: 1024
num_epochs: 10
learning_rate: 0.0003
corpus_split:
  general_web: 0.30
  books: 0.15
  academic_papers: 0.20
  code: 0.09
  math_technical_reasoning: 0.20
  warmup: 0.01
  conversation_forum: 0.05